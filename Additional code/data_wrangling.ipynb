{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyOeiYN4eE9im8BRIuRU15CV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Setting working directory"],"metadata":{"id":"R9e8byNPg2fN"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive/')\n","%cd /content/drive/MyDrive/DATA309 Capstone Project - Lego4/write ups/Additional files/"],"metadata":{"id":"bOnLnFYsg00T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Retrieving and processing (sentiment analysis) of brickset review data"],"metadata":{"id":"ZRfwvJNEeyhk"}},{"cell_type":"markdown","source":["The code below was used to retrieve the reviews form every set on brickset, the start and end indexs were updated accordingly as each chunk was complete (note that this code is in r, not python)"],"metadata":{"id":"_C3Eh29ge7qr"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"hx0Uw-BtceqM"},"outputs":[],"source":["allreviewdata <- data.frame()\n","broken_reviews <- c('306')\n","\n","start_index <- 14001\n","end_index <- 19409\n","\n","# Loop through the sets\n","for (i in start_index:end_index) {\n","  if (legosets$reviewCount[i] > 0 & !(i %in% broken_reviews)) {\n","\n","    setid <- legosets$setID[i]\n","\n","    setreviewdata <- getReviews(setid)\n","\n","    setreviewdata$setID <- rep(setid, nrow(setreviewdata))\n","\n","    allreviewdata <- rbind(allreviewdata, setreviewdata)\n","  }\n","}\n","\n","# View the combined review data\n","print(allreviewdata)\n","\n","write.csv(allreviewdata, file = f\"{start_index}z{end_index}reviews.csv\", row.names = TRUE)"]},{"cell_type":"markdown","source":["The following code combined all perviously collected review data, then combining and exporting this all as one file"],"metadata":{"id":"WEdrJsKgfDN8"}},{"cell_type":"code","source":["import csv\n","\n","review_files = [\"1z2000reviews.csv\", \"2001z4000reviews.csv\", \"4001z6000reviews.csv\", \"6001z10000reviews.csv\", \"10001z14000reviews.csv\", \"14001z19409reviews.csv\"]\n","\n","all_data = []\n","for i in range(len(review_files)):\n","  file_to_process = review_files[i]\n","  with open(file_to_process, newline='') as csvfile:\n","    reader = csv.reader(csvfile)\n","    if i == 0:\n","      for row in reader:\n","        all_data.append(row)\n","    else:\n","      next(reader)\n","      for row in reader:\n","        all_data.append(row)\n","\n","import pandas as pd\n","\n","newdata = pd.DataFrame(all_data[1:], columns=all_data[0])\n","\n","newdata.to_csv(\"all_reviews_combined.csv\", index=False)"],"metadata":{"id":"Ls4Pc9lse6lD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Inialise sentiment analysis"],"metadata":{"id":"08wpfn8FfVv7"}},{"cell_type":"code","source":["file_to_process = \"all_reviews_combined.csv\" #NEED TO CHANGE THIS DEPENDING ON WHAT YOU WANNA UPDATE\n","data_list = []\n","with open(file_to_process, newline='') as csvfile:\n","    reader = csv.reader(csvfile)\n","    for row in reader:\n","        data_list.append(row)\n","\n","import nltk\n","from nltk.sentiment import SentimentIntensityAnalyzer\n","\n","nltk.download('vader_lexicon')\n","sia = SentimentIntensityAnalyzer()"],"metadata":{"id":"cFvY-eOLfMdz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Complete sentiment analysis using nltk on all brickset reviews and exporting it"],"metadata":{"id":"VXtRIkNlfe2r"}},{"cell_type":"code","source":["data_list[0] = data_list[0][:12]\n","title_headers = [\"title_neg\", \"title_neu\", \"title_pos\", \"title_comp\"]\n","review_headers = [\"review_neg\", \"review_neu\", \"review_pos\", \"review_comp\"]\n","data_list[0] += title_headers + review_headers\n","data_list[0] = data_list[0][:20]\n","\n","for i in range(1, len(data_list)):\n","  data_list[i] = data_list[i][:12]\n","  ts = sia.polarity_scores(data_list[i][3])\n","  rs = sia.polarity_scores(data_list[i][4])\n","  ts_list = [ts['neg'], ts['neu'], ts['pos'], ts['compound']]\n","  rs_list = [rs['neg'], rs['neu'], rs['pos'], rs['compound']]\n","  data_list[i] += ts_list + rs_list\n","  data_list[i] = data_list[i][:20]\n","\n","\n","newdata = pd.DataFrame(data_list[1:], columns=data_list[0])\n","\n","newdata.to_csv(\"brickset all reviews\", index=False)"],"metadata":{"id":"jDLFIQLUfUrj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Adding sentiment analysis back to brickset dataset"],"metadata":{"id":"tmW8QTdbi0iC"}},{"cell_type":"code","source":["import csv\n","\n","file_to_process1 = \"brickset all reviews.csv\"\n","file_to_process2 = \"brickset legoset data.csv\"\n","all_reviews_data = []\n","brickset_data = []\n","with open(file_to_process1, newline='') as csvfile:\n","    reader = csv.reader(csvfile)\n","    for row in reader:\n","        all_reviews_data.append(row)\n","with open(file_to_process2, newline='') as csvfile:\n","    reader = csv.reader(csvfile)\n","    for row in reader:\n","        brickset_data.append(row)"],"metadata":{"id":"xhZKof6vij8K"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Making missing data easier to handle"],"metadata":{"id":"zOYdhsOtgqnC"}},{"cell_type":"code","source":["for i in range(1, len(brickset_data)):\n","  for j in range(1, len(brickset_data[i])):\n","    if brickset_data[i][j] == 'NA':\n","      brickset_data[i][j] = None\n","\n","for i in range(1, len(all_reviews_data)):\n","  for j in range(6, 11):\n","    if all_reviews_data[i][j] == '0':\n","      all_reviews_data[i][j] = None"],"metadata":{"id":"zzB3sMhAgqL6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Finding reviews per set"],"metadata":{"id":"eSmKy5s-ieVT"}},{"cell_type":"code","source":["review_dict = {}\n","review_dict['setID'] = all_reviews_data[0][6:11] + all_reviews_data[0][12:]\n","for i in range(1, len(all_reviews_data)):\n","  setID = all_reviews_data[i][11]\n","  review_dict[setID] = review_dict.get(setID, []) + [all_reviews_data[i][6:11] + all_reviews_data[i][12:]]"],"metadata":{"id":"01-rcX1TidyS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Finding the average sentiment scores of each lego sets reviews and then exporting this as the new usable brickset dataset"],"metadata":{"id":"-b4_OkUAi9lS"}},{"cell_type":"code","source":["review_avgs = [[\"setID\", \"num_reviews\"] + [review_dict['setID'][i] for i in range(13)]]  # Assuming 13 columns from review_dict\n","\n","fill_in = [None] * 13  # Adjusting the fill_in to match the number of columns\n","\n","def avg_ith_column_noskip(data, i):\n","    total = 0\n","    count = 0\n","    for entry in data:\n","        total += float(entry[i])\n","        count += 1\n","    return total / count\n","\n","def avg_ith_column_skips(data, i):\n","    total = 0\n","    count = 0\n","    for entry in data:\n","        if entry[i] is not None:\n","          total += float(entry[i])\n","          count += 1\n","    if count == 0:\n","        return None\n","    else:\n","      return total / count\n","\n","for i in range(1, len(brickset_data)):\n","    setID = brickset_data[i][0]\n","    count = len(review_dict.get(setID, []))\n","    row = [setID, count]\n","\n","    if count > 0:\n","        for j in range(5):\n","            result = avg_ith_column_skips(review_dict.get(setID, []), j)\n","            row.append(result)\n","        for j in range(5, 13):\n","            result = avg_ith_column_noskip(review_dict.get(setID, []), j)\n","            row.append(result)\n","        review_avgs.append(row)\n","    else:\n","        review_avgs.append(row + fill_in)\n","\n","import pandas as pd\n","\n","newdata1 = pd.DataFrame(review_avgs[1:], columns=review_avgs[0])\n","\n","newdata1.to_csv(\"usable_review_data.csv\", index=False)\n","\n","newdata2 = pd.DataFrame(brickset_data[1:], columns=brickset_data[0])\n","\n","newdata2.to_csv(\"usable_brickset_data.csv\", index=False)"],"metadata":{"id":"NCzBFmUri9PS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Handling retail and market price data"],"metadata":{"id":"72OOhWIZjTh6"}},{"cell_type":"markdown","source":["Exploring availability of retail price data"],"metadata":{"id":"Wd5SFMcvjSZh"}},{"cell_type":"code","source":["US_Price = set()\n","UK_Price = set()\n","CA_Price = set()\n","DE_Price = set()\n","all_prices = set()\n","\n","for i in range(1, len(brickset_data)):\n","    for j in [20, 23, 26, 29]:\n","      if brickset_data[i][j] != '':\n","        if j == 20:\n","          US_Price.add(brickset_data[i][1])\n","        if j == 23:\n","          UK_Price.add(brickset_data[i][1])\n","        if j == 26:\n","          CA_Price.add(brickset_data[i][1])\n","        if j == 29:\n","          DE_Price.add(brickset_data[i][1])\n","        all_prices.add(brickset_data[i][1])\n","\n","print(len(US_Price), len(UK_Price), len(CA_Price), len(DE_Price))\n","print(len(all_prices))\n","\n","US_all_difference = all_prices.difference(US_Price)\n","print(\"\")\n","print(len(US_all_difference))\n","print(len(UK_Price.intersection(US_all_difference)), len(CA_Price.intersection(US_all_difference)),len(DE_Price.intersection(US_all_difference)))"],"metadata":{"id":"6YgKbzSmjSFC"},"execution_count":null,"outputs":[]}]}